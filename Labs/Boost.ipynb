{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: PA - XGBoost\n",
    "author: Summer Churby\n",
    "format: \n",
    "        html: \n",
    "            toc: true\n",
    "            code-fold: true\n",
    "embed-resources: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should compare XGBoost or Gradient Boosting to the results of your previous AdaBoost activity.\n",
    "Based on the visualizations seen at the links above you're probably also thinking that this classification task should not be that difficult. So, a secondary goal of this assignment is to test the effects of the XGBoost (or Gradient Boosting) function arguments on the algorithm's performance. \n",
    "You should explore at least 3 different sets of settings for the function inputs, and you should do your best to find values for these inputs that actually change the results of your modelling. That is, try not to run three different sets of inputs that result in the same performance. The goal here is for you to better understand how to set these input values yourself in the future. Comment on what you discover about these inputs and how the behave.\n",
    "Your submission should be built and written with non-experts as the target audience. All of your code should still be included, but do your best to narrate your work in accessible ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "import pandas as pd\n",
    "penguins = pd.read_csv(r\"C:\\Users\\achur\\OneDrive\\Desktop\\School\\CP Spring 2024\\545\\GSB545\\Labs\\penguins_size.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboostNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading xgboost-3.0.0-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\achur\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xgboost) (2.2.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\achur\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xgboost) (1.15.2)\n",
      "Downloading xgboost-3.0.0-py3-none-win_amd64.whl (150.0 MB)\n",
      "   ---------------------------------------- 0.0/150.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 7.9/150.0 MB 40.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 15.7/150.0 MB 38.1 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 23.6/150.0 MB 37.3 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 30.4/150.0 MB 35.1 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 38.0/150.0 MB 34.0 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 46.1/150.0 MB 34.2 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 53.2/150.0 MB 33.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 60.0/150.0 MB 33.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 67.6/150.0 MB 34.0 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 74.2/150.0 MB 33.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 81.0/150.0 MB 33.6 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 88.3/150.0 MB 33.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 95.4/150.0 MB 33.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 101.7/150.0 MB 33.0 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 108.8/150.0 MB 32.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 116.4/150.0 MB 33.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 122.9/150.0 MB 32.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 129.8/150.0 MB 32.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 136.1/150.0 MB 32.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 142.9/150.0 MB 32.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 32.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 32.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 32.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 32.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 32.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 32.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 32.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 32.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 32.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 150.0/150.0 MB 22.8 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\achur\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Test Accuracy: 97.01 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:50:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "# first xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Drop missing values\n",
    "penguins = penguins.dropna()\n",
    "\n",
    "# Encode all categorical variables\n",
    "for label in penguins.columns:\n",
    "    penguins[label] = LabelEncoder().fit_transform(penguins[label])\n",
    "\n",
    "# Split features and target\n",
    "X = penguins.drop(['species'], axis=1)\n",
    "Y = penguins['species']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state = 4)\n",
    "\n",
    "# XGBoost model\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=7)\n",
    "\n",
    "# Fit model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"XGBoost Test Accuracy:\", round(accuracy * 100, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:47:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 2 Accuracy: 98.51 %\n"
     ]
    }
   ],
   "source": [
    "# second xgboost\n",
    "xgb2 = XGBClassifier(max_depth=2, learning_rate=0.1, n_estimators=200, subsample=0.8,\n",
    "                     colsample_bytree=0.8, use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "xgb2.fit(X_train, y_train)\n",
    "acc2 = accuracy_score(y_test, xgb2.predict(X_test))\n",
    "print(\"Set 2 Accuracy:\", round(acc2 * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achur\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [08:57:36] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 3 Accuracy: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "xgb3 = XGBClassifier(\n",
    "    max_depth=1,\n",
    "    learning_rate=0.5,\n",
    "    n_estimators=1000,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.6,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=15\n",
    ")\n",
    "\n",
    "xgb3.fit(X_train, y_train)\n",
    "acc3 = accuracy_score(y_test, xgb3.predict(X_test))\n",
    "print(\"Set 3 Accuracy:\", round(acc3 * 100, 2), \"%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each different model that was run, all of the predicted accuracies were very close. For the first model, I ran a straight xgboosting classifier and got a 97.01% accuracy. For the second model, I made the max_depth and the learning rate lower and got an accuracy rate of 98.51%. For the third model, I made the max_depth even lower and increased the estimators and got an accuracy rate of 100%. In comparison to the adaboost models, all 3 of the adaboost models produced accuracy rates of 98.057%, 97.60%, and 98.8%. On average, xgboost predicts at a higher accuracy rate. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
